import argparse
import os
import glob
import json

import erniebot
import gradio as gr
from pipelines.document_stores import FAISSDocumentStore
from pipelines.nodes import (
    CharacterTextSplitter,
    SpacyTextSplitter,
    EmbeddingRetriever,
    ErnieBot,
    ErnieRanker,
    PDFToTextConverter,
    PromptTemplate,
)
from pipelines.pipelines import Pipeline

parser = argparse.ArgumentParser()
parser.add_argument("--index_name", default='construct_demo_index', type=str, help="The ann index name of ANN.")
parser.add_argument("--file_paths", default='./construction_regulations', type=str, help="The PDF file path.")
parser.add_argument("--retriever_top_k", default=5, type=int, help="Number of recall items for search")
parser.add_argument("--chunk_size", default=384, type=int, help="The length of data for indexing by retriever")
parser.add_argument('--host', type=str, default="localhost", help='host ip of ANN search engine')
parser.add_argument('--port', type=int, default=8081, help='host ip of ANN search engine')
parser.add_argument("--api_key", default=None, type=str, help="The API Key.")
parser.add_argument("--secret_key", default=None, type=str, help="The secret key.")
args = parser.parse_args()

erniebot.api_type = "qianfan"
erniebot.ak = args.api_key
erniebot.sk = args.secret_key

# åˆ©ç”¨Paddle-Pipelinesæ„å»ºæœ¬åœ°è¯­ä¹‰æ£€ç´¢æœåŠ¡
faiss_document_store = "faiss_document_store.db"
# å¦‚æœ¬åœ°è¯­ä¹‰æ£€ç´¢ç´¢å¼•å·²ç»å­˜åœ¨ï¼Œåˆ™ç›´æ¥è¯»å–å·²ç»æ„å»ºå¥½çš„ç´¢å¼•
if os.path.exists(args.index_name) and os.path.exists(faiss_document_store):
    # connect to existed FAISS Index
    document_store = FAISSDocumentStore.load(args.index_name)
    retriever = EmbeddingRetriever(
        document_store=document_store,
        retriever_batch_size=16,  # 16 is the max batch size allowed by ErnieBot Embedding
        api_key=args.api_key,
        secret_key=args.secret_key,
    )
# å¦‚æœ¬åœ°è¯­ä¹‰æ£€ç´¢ç´¢å¼•å·²ç»å­˜åœ¨ï¼Œåˆ™ç›´æ¥è¯»å–å·²ç»æ„å»ºå¥½çš„ç´¢å¼•
else:
    if os.path.exists(args.index_name):
        os.remove(args.index_name)
    if os.path.exists(faiss_document_store):
        os.remove(faiss_document_store)
    document_store = FAISSDocumentStore(
        embedding_dim=384,  # hardcode the embedding dim to 384 for ErnieBot Embedding
        duplicate_documents="skip",
        return_embedding=True,
        faiss_index_factory_str="Flat",
    )
    retriever = EmbeddingRetriever(
        document_store=document_store,
        retriever_batch_size=16,  # 16 is the max batch size allowed by ErnieBot Embedding
        api_key=args.api_key,
        secret_key=args.secret_key,
    )
    # å°†Wordæ–‡æ¡£è½¬æ¢ä¸ºæ–‡å­—
    pdf_converter = PDFToTextConverter()
    # å°†æ–‡å­—åˆ†ç‰‡
    text_splitter = SpacyTextSplitter(separator="\n", chunk_size=384, chunk_overlap=128, filters=["\n"])
    indexing_pipeline = Pipeline()
    indexing_pipeline.add_node(component=pdf_converter, name="pdf_converter", inputs=["File"])
    indexing_pipeline.add_node(component=text_splitter, name="Splitter", inputs=["pdf_converter"])
    indexing_pipeline.add_node(component=retriever, name="Retriever", inputs=["Splitter"])
    indexing_pipeline.add_node(component=document_store, name="DocumentStore", inputs=["Retriever"])
    files_paths = glob.glob(args.file_paths + "/*.pdf")
    indexing_pipeline.run(file_paths=files_paths)
    document_store.save(args.index_name)

# æ„å»ºç”¨äºæŸ¥è¯¢çš„Pipelines
ernie_bot = ErnieBot(api_key=args.api_key, secret_key=args.secret_key)
query_pipeline = Pipeline()
query_pipeline.add_node(component=retriever, name="Retriever", inputs=["Query"])

# å®šä¹‰å‡½æ•°api, å…±1ä¸ªapi, ä»¥åŠ2ä¸ªä½¿ç”¨ä¾‹å­
functions = [
    {
        "name": "search_knowledge_base",
        "description": "åœ¨ä½æˆ¿å’ŒåŸä¹¡å»ºè®¾éƒ¨è§„ç« ä¸­å¯»æ‰¾å’Œqueryæœ€ç›¸å…³çš„ç‰‡æ®µ",
        "parameters": {
            "type": "object",
            "properties": {"query": {"type": "string", "description": "è§„ç« æŸ¥è¯¢è¯­å¥"}},
            "required": ["query"],
        },
        "responses": {
            "type": "object",
            "description": "æ£€ç´¢ç»“æœï¼Œå†…å®¹ä¸ºä½æˆ¿å’ŒåŸä¹¡å»ºè®¾éƒ¨è§„ç« ä¸­å’Œqueryç›¸å…³çš„æ–‡å­—ç‰‡æ®µ",
            "properties": {
                "documents": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "document": {"type": "string", "description": "å’Œqueryç›¸å…³çš„æ–‡å­—ç‰‡æ®µ"},
                        },
                    },
                }
            },
            "required": ["documents"],
        },
        "examples": [
            {"role": "user", "content": "ä¼ä¸šç”³è¯·å»ºç­‘ä¸šä¼ä¸šèµ„è´¨éœ€è¦å“ªäº›ææ–™ï¼Ÿ"},
            {
                "role": "assistant",
                "content": None,
                "function_call": {
                    "name": "search_knowledge_base",
                    "thoughts": "è¿™æ˜¯å’ŒåŸå¸‚å»ºè®¾æ³•è§„æ ‡å‡†ç›¸å…³çš„é—®é¢˜ï¼Œæˆ‘éœ€è¦æŸ¥è¯¢ä½æˆ¿å’ŒåŸä¹¡å»ºè®¾éƒ¨è§„ç« ï¼Œå¹¶ä¸”è®¾ç½®queryä¸º'ä¼ä¸šç”³è¯·å»ºç­‘ä¸šä¼ä¸šèµ„è´¨éœ€è¦çš„ææ–™'",
                    "arguments": '{ "query": "ä¼ä¸šç”³è¯·å»ºç­‘ä¸šä¼ä¸šèµ„è´¨éœ€è¦å“ªäº›ææ–™ï¼Ÿ"}',
                },
            },
            {"role": "user", "content": "å†å²æ–‡åŒ–è¡—åŒºçš„åŸå¸‚è®¾è®¡æœ‰ä»€ä¹ˆè¦æ±‚ï¼Ÿ"},
            {
                "role": "assistant",
                "content": None,
                "function_call": {
                    "name": "search_knowledge_base",
                    "thoughts": "è¿™æ˜¯å’ŒåŸå¸‚å»ºè®¾æ³•è§„æ ‡å‡†ç›¸å…³çš„é—®é¢˜ï¼Œæˆ‘éœ€è¦æŸ¥è¯¢ä½æˆ¿å’ŒåŸä¹¡å»ºè®¾éƒ¨è§„ç« ï¼Œå¹¶ä¸”è®¾ç½®queryä¸º'å†å²æ–‡åŒ–è¡—åŒºçš„è®¾è®¡è¦æ±‚'",
                    "arguments": '{ "query": "å†å²æ–‡åŒ–è¡—åŒºçš„è®¾è®¡è¦æ±‚"}',
                },
            },
        ],
    }
]


def search_knowledge_base(query):
    prediction = query_pipeline.run(
        query=query,
        params={
            "Retriever": {
                "top_k": args.retriever_top_k,
            },
        },
    )
    documents = [{ "document": doc.content } for doc in prediction["documents"]]
    return {"documents": documents}

def history_transform(history=[]):
    messages = []
    if len(history) < 2:
        return messages

    for turn_idx in range(1, len(history)):
        messages.extend(
            [{"role": "user", "content": history[turn_idx][0]}, {"role": "assistant", "content": history[turn_idx][1]}]
        )
    return messages


def add_message_chatbot(messages, history):
    history.append([messages, None])
    return None, history


def prediction(history):
    logs = []
    query = history.pop()[0]
    if query == "":
        return history, "æ³¨æ„ï¼šé—®é¢˜ä¸èƒ½ä¸ºç©º"
    
    # æ¶ˆé™¤æ½œåœ¨çš„é”™è¯¯æ ¼å¼é—®é¢˜
    for turn_idx in range(len(history)):
        if history[turn_idx][0] is not None:
            history[turn_idx][0] = history[turn_idx][0].replace("<br>", "")
        if history[turn_idx][1] is not None:
            history[turn_idx][1] = history[turn_idx][1].replace("<br>", "")

    # å°†å¯¹è¯å†å²ä»gradioæ ¼å¼è½¬åŒ–ä¸º eb sdkçš„æ ¼å¼
    messages = history_transform(history)
    # æ’å…¥å°†å½“å‰è½®æ¬¡çš„ç”¨æˆ·queryæ’å…¥ä¸Šä¸‹æ–‡å½“ä¸­
    messages.append({"role": "user", "content": query})
    # è°ƒç”¨ebçš„chat completion, æä¾›functionså…¥å‚
    response = erniebot.ChatCompletion.create(
        model="ernie-bot-3.5",
        messages=messages,
        functions=functions,
    )
    # å¦‚æœfunction callæœªè§¦å‘ï¼Œæ¨¡å‹ç›´æ¥å›ç­”ï¼Œåˆ™ç›´æ¥è¿”å›æ¨¡å‹ç»“æœ
    if "function_call" not in response:
        logs.append({"function_callç»“æœ": "æœªè§¦å‘"})
        result = response["result"]
    # å¦‚æœfunction callè§¦å‘
    else:
        function_call = response.function_call
        logs.append({"function_callç»“æœ": function_call})
        # è§£ææ¨¡å‹è¿”å›çš„function callå…¥å‚
        func_args = json.loads(function_call["arguments"])
        # è°ƒç”¨function
        res = search_knowledge_base(**func_args)
        logs.append({"æ£€ç´¢ç»“æœ": res})
        # ä½¿ç”¨ ebçš„chat completion å¯¹äºfunctionè¿”å›çš„ç»“æœè¿›è¡Œæ¶¦è‰², ç”¨ç»“æœå›å¤ç”¨æˆ·
        messages.append({"role": "assistant", "content": None, "function_call": function_call})
        messages.append(
            {"role": "function", "name": function_call["name"], "content": json.dumps(res, ensure_ascii=False)}
        )
        response = erniebot.ChatCompletion.create(model="ernie-bot-3.5", messages=messages)
        result = response["result"]
    history.append([query, result])
    return history, logs


def launch_ui():
    with gr.Blocks(title="ERNIE Bot åŸå¸‚å»ºè®¾æ³•è§„æ ‡å‡†å°åŠ©æ‰‹", theme=gr.themes.Base()) as demo:
        gr.HTML("""<h1 align="center">ERNIE Bot åŸå¸‚å»ºè®¾æ³•è§„æ ‡å‡†å°åŠ©æ‰‹</h1>""")
        with gr.Column():
            chatbot = gr.Chatbot(value=[[None, "æ‚¨å¥½, æˆ‘æ˜¯ ERNIE Bot åŸå¸‚å»ºè®¾æ³•è§„æ ‡å‡†å°åŠ©æ‰‹ã€‚é™¤äº†æ™®é€šçš„å¤§æ¨¡å‹èƒ½åŠ›ä»¥å¤–ï¼Œè¿˜ç‰¹åˆ«äº†è§£ä½æˆ¿å’ŒåŸä¹¡å»ºè®¾éƒ¨è§„ç« å“¦"]], scale=35, height=500)
            message = gr.Textbox(placeholder="å“ªäº›å»ºç­‘ä¼ä¸šèµ„è´¨éœ€è¦å›½åŠ¡é™¢ä½æˆ¿åŸä¹¡å»ºè®¾ä¸»ç®¡éƒ¨é—¨è®¸å¯ï¼Ÿ", lines=1, max_lines=20)
            with gr.Row():
                submit = gr.Button("ğŸš€ æäº¤", variant="primary", scale=1)
                clear = gr.Button("æ¸…é™¤", variant="primary", scale=1)
            log = gr.JSON()
        message.submit(add_message_chatbot, inputs=[message, chatbot], outputs=[message, chatbot]).then(
            prediction, inputs=[chatbot], outputs=[chatbot, log]
        )
        submit.click(add_message_chatbot, inputs=[message, chatbot], outputs=[message, chatbot]).then(
            prediction, inputs=[chatbot], outputs=[chatbot, log]
        )
        clear.click(lambda _: ([[None, "æ‚¨å¥½, æˆ‘æ˜¯ ERNIE Bot åŸå¸‚å»ºè®¾æ³•è§„æ ‡å‡†å°åŠ©æ‰‹ã€‚é™¤äº†æ™®é€šçš„å¤§æ¨¡å‹èƒ½åŠ›ä»¥å¤–ï¼Œè¿˜ç‰¹åˆ«äº†è§£ä½æˆ¿å’ŒåŸä¹¡å»ºè®¾éƒ¨è§„ç« å“¦"]]), inputs=[clear], outputs=[chatbot])
    demo.launch(server_name=args.host, server_port=args.port, debug=True)

if __name__ == "__main__":
    launch_ui()
